{"cells":[{"cell_type":"markdown","metadata":{"id":"AmbGxvlH1DIy"},"source":["Loads and Validates Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bh5ueaNmf8WX"},"outputs":[],"source":["# Neural Networks\n","\n","# Class Notes:\n","\n","# predict total number of bike rentals that occured?\n","# y is casual and registered combined.\n","# how do we want the data fed into the network?\n","# how will we use the data from 2020, the lockdown?\n","# combine values from hum and temp_c, feature enginere road conditions.\n","# trend from weather and covid.\n","# What were the exact dates the DC lockdown?\n","# frequency, average trend of busier times\n","# look at the tourist Season\n","# use both temp_c and temp_f"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"mA0HPVmIBT4C"},"outputs":[],"source":["import pandas as pd\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import matplotlib.pyplot as plt\n","\n","bikes = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes.csv')\n","mini_holdout = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/biking_holdout_test_mini.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tcxEK4p03B5T"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YQCclVUXylwJ"},"outputs":[],"source":["def get_day_of_year(row):\n","  date_list = row[\"dteday\"].split(\"/\")\n","\n","  month = int(date_list[0])\n","  day = int(date_list[1])\n","  year = int(date_list[2]) # check for leap years\n","\n","  if month == 1:\n","    days = day\n","  elif month == 2:\n","    days = day + 31\n","  elif month == 3:\n","    days = day + 31 + 28\n","  elif month == 4:\n","    days = day + 31 + 28 + 31\n","  elif month == 5:\n","    days = day + 31 + 28 + 31 + 30\n","  elif month == 6:\n","    days = day + 31 + 28 + 31 + 30 + 31\n","  elif month == 7:\n","    days = day + 31 + 28 + 31 + 30 + 31 + 30\n","  elif month == 8:\n","    days = day + 31 + 28 + 31 + 30 + 31 + 30 + 31\n","  elif month == 9:\n","    days = day + 31 + 28 + 31 + 30 + 31 + 30 + 31 + 31\n","  elif month == 10:\n","    days = day + 31 + 28 + 31 + 30 + 31 + 30 + 31 + 31 + 30\n","  elif month == 11:\n","    days = day + 31 + 28 + 31 + 30 + 31 + 30 + 31 + 31 + 30 + 31\n","  elif month == 12:\n","    days = day + 31 + 28 + 31 + 30 + 31 + 30 + 31 + 31 + 30 + 31 + 30\n","\n","  if year % 4 == 0:\n","    days += 1\n","\n","  return int(days)\n","\n","def get_day_of_four_years(row):\n","  # there are 1461 days in four years but I want everything to be centered around election year\n","  year = int(row[\"dteday\"].split(\"/\")[2])\n","  day = int(row[\"dteday\"].split(\"/\")[1])\n","\n","  days = ((year % 4) * 365) + day\n","  if year % 4 != 0:\n","    days += 1\n","\n","  return int(days)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-zqoaAwl_IRP"},"outputs":[],"source":["def preprocess_bike_data(df):\n","    df[\"day_of_year\"] = df.apply(get_day_of_year, axis=1)\n","    df[\"day_of_four_years\"] = df.apply(get_day_of_four_years, axis=1)\n","\n","    # Convert 'dteday' to datetime\n","    df['dteday'] = pd.to_datetime(df['dteday'], format='%m/%d/%Y')\n","\n","    # Ensure 'hr' is an integer\n","    df['hr'] = df['hr'].astype(int)\n","\n","    # Combine 'dteday' and 'hr' into a single 'datetime' column\n","    df['dteday'] = df.apply(lambda row: pd.Timestamp(row['dteday']) + pd.Timedelta(hours=row['hr']), axis=1)\n","\n","    # Create year, month, and day columns\n","    df['year']        = df['dteday'].dt.year\n","    df['month']       = df['dteday'].dt.month\n","    df['day']         = df['dteday'].dt.day\n","    df['day_of_week'] = df['dteday'].dt.dayofweek\n","\n","    return df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"eKnaaDNqdtJp"},"outputs":[],"source":["bikes = preprocess_bike_data(bikes)\n","\n","bikes.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"prcujQMQNwVw"},"outputs":[],"source":["no_covid = bikes\n","from_ts = pd.to_datetime('2020-03-01 00:00:00')\n","to_ts = pd.to_datetime('2021-03-31 00:00:00')\n","no_covid = no_covid[(no_covid['dteday'] < from_ts) | (no_covid['dteday'] > to_ts)]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"cW9wgXILMVQO"},"outputs":[],"source":["'''\n","This is a chronological train/ test split.\n","'''\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","\n","# Get dummies\n","#bikes_dum = pd.get_dummies(bikes, columns=['hr', 'weathersit', 'season', 'holiday', 'workingday', 'year', 'month', 'day'])\n","bikes_dum = bikes\n","\n","# Ensure the data is sorted by 'datetime' to maintain chronological order\n","bikes_dum = bikes_dum.sort_values(by='dteday')\n","\n","# Create the target variable\n","bikes_dum['combined_y'] = bikes_dum['casual'] + bikes_dum['registered']\n","y = bikes_dum['combined_y']\n","\n","# Define indices for chronological splitting\n","split_index = int(len(bikes_dum) * 0.9)\n","\n","# Split the data chronologically\n","# Get the size of bikes for train/validation/testing\n","train_size = int(0.9 * len(bikes))\n","val_size = int(0.05 * len(bikes))\n","test_size = len(bikes) - train_size - val_size\n","\n","# Split the data for train/val/test\n","train_data = bikes_dum[:train_size]\n","val_data = bikes_dum[train_size:train_size + val_size]\n","test_data = bikes_dum[train_size + val_size:]\n","\n","# Separate features and target for training and testing sets\n","y_train = train_data['combined_y']\n","y_test = test_data['combined_y']\n","y_val = val_data['combined_y']\n","\n","X_train = train_data.drop(['combined_y', 'dteday', 'casual', 'registered'], axis=1)\n","X_test = test_data.drop(['combined_y', 'dteday', 'casual', 'registered'], axis=1)\n","X_val = val_data.drop(['combined_y', 'dteday', 'casual', 'registered'], axis=1)\n","\n","# Scale the data\n","minmax_scaler = preprocessing.MinMaxScaler()\n","X_train = minmax_scaler.fit_transform(X_train)\n","X_test = minmax_scaler.transform(X_test)\n","\n","# Display the shapes of the datasets to verify\n","print(f'Training data shape: {X_train.shape}, Training target shape: {y_train.shape}')\n","print(f'Testing data shape: {X_test.shape}, Testing target shape: {y_test.shape}')\n","print(f'Validation data shape: {X_val.shape}, Validation target shape: {y_val.shape}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uYDGf07jORbu"},"outputs":[],"source":["'''\n","no_covid = no_covid.sort_values(by='dteday')\n","\n","# Create the target variable\n","no_covid['combined_y'] = no_covid['casual'] + no_covid['registered']\n","y = no_covid['combined_y']\n","\n","# Define indices for chronological splitting\n","split_index = int(len(no_covid) * 0.9)\n","\n","# Split the data chronologically\n","# Get the size of bikes for train/validation/testing\n","train_size2 = int(0.9 * len(bikes))\n","val_size = int(0.05 * len(bikes))\n","test_size = len(bikes) - train_size - val_size\n","\n","# Split the data for train/val/test\n","train_data2 = no_covid[:train_size]\n","val_data2 = no_covid[train_size:train_size + val_size]\n","test_data2 = no_covid[train_size + val_size:]\n","\n","# Separate features and target for training and testing sets\n","y_train2 = train_data2['combined_y']\n","y_test2 = test_data2['combined_y']\n","y_val2 = val_data2['combined_y']\n","\n","X_train2 = train_data2.drop(['combined_y', 'dteday', 'casual', 'registered'], axis=1)\n","X_test2 = test_data2.drop(['combined_y', 'dteday', 'casual', 'registered'], axis=1)\n","X_val2 = val_data2.drop(['combined_y', 'dteday', 'casual', 'registered'], axis=1)\n","\n","# Scale the data\n","minmax_scaler2 = preprocessing.MinMaxScaler()\n","X_train2 = minmax_scaler2.fit_transform(X_train)\n","X_test2 = minmax_scaler2.transform(X_test)\n","\n","# Display the shapes of the datasets to verify\n","print(f'Training data shape: {X_train2.shape}, Training target shape: {y_train2.shape}')\n","print(f'Testing data shape: {X_test2.shape}, Testing target shape: {y_test2.shape}')\n","print(f'Validation data shape: {X_val2.shape}, Validation target shape: {y_val2.shape}')\n","'''"]},{"cell_type":"markdown","metadata":{"id":"WYGuq2Vc7RJ0"},"source":["**Load Models**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R-JBSQ5E7TL7"},"outputs":[],"source":["!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gw7rPsob1Nxe"},"outputs":[],"source":["from google.colab import drive\n","import os\n","from keras.models import load_model\n","import tensorflow as tf\n","\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"43P7ZocG2hp7"},"outputs":[],"source":["# I'm getting an error, I'm trying again with a .h5 file\n","top_file = \"/content/drive/My Drive/Machine Learning Group 5/Bikes (Neural Nets)/\"\n","sigmoid = tf.keras.models.load_model(top_file + 'sigmoid.h5')\n","tanh = tf.keras.models.load_model(top_file + 'tanh.h5')\n","# relu = tf.keras.models.load_model(top_file + 'my_model.keras')\n","# sigmoid_no_covid = tf.keras.models.load_model(top_file + 'sigmoid_no_covid.h5')\n","# tanh_no_covid = tf.keras.models.load_model(top_file + 'tanh_no_covid.h5')"]},{"cell_type":"markdown","metadata":{"id":"qftGk0j-kWcd"},"source":["Test Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1JclhAtylM_R"},"outputs":[],"source":["from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"74NrJY59kVZo"},"outputs":[],"source":["def get_predict(model):\n","  predictions = np.round(model.predict(X_test),1)\n","  return predictions\n","\n","def get_r(predictions, y_test):\n","  r2 = r2_score(y_test,predictions)\n","  return r2\n","\n","def get_mse(predictions, y_test):\n","  MSE = mean_squared_error(y_test,predictions)\n","  return MSE\n","\n","def get_rmse(predictions, y_test):\n","  rmse = np.sqrt(get_mse(predictions, y_test))\n","  return rmse\n","\n","def get_mean_ae(predictions, y_test):\n","  return mean_absolute_error(y_test,predictions)\n","\n","def get_median_ae(predictions, y_test):\n","  return median_absolute_error(y_test,predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bjTpaGo0L3rD"},"outputs":[],"source":["def get_scores(predictions, y_test):\n","  print(\"R2: \", get_r(predictions, y_test))\n","  print(\"MSE: \", get_mse(predictions, y_test))\n","  print(\"RMSE: \", get_rmse(predictions, y_test))\n","  print(\"Mean Absolute: \", get_mean_ae(predictions, y_test))\n","  print(\"Median Absolute: \", get_median_ae(predictions,y_test))\n"]},{"cell_type":"markdown","metadata":{"id":"XmTEvG2JJURF"},"source":["Hold Out Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RJDHdqsdqDz0"},"outputs":[],"source":["# Load and preprocess mini_holdout\n","# mini_holdout = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/biking_holdout_test_mini.csv')\n","mini_holdout = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes_december.csv')\n","mini_holdout = preprocess_bike_data(mini_holdout)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NmjiV_irr5uU"},"outputs":[],"source":["# Ensure that mini_holdout has the same columns as the training data\n","missing_cols = set(bikes.columns) - set(mini_holdout.columns)\n","for col in missing_cols:\n","    mini_holdout[col] = 0\n","\n","# Drop unnecessary columns in mini_holdout\n","mini_holdout = mini_holdout.drop(['dteday', 'datetime'], axis=1, errors='ignore')\n","\n","# Align columns to match the training data\n","mini_holdout = mini_holdout[bikes.columns.drop(['dteday', 'casual', 'registered'])]\n","\n","# Apply the same scaler fitted on the training data\n","mini_holdout = minmax_scaler.transform(mini_holdout)\n","\n","# Check the shape of mini_holdout\n","print(mini_holdout.shape)\n","\n","# Make predictions\n","mini_holdout_predictions = sigmoid.predict(mini_holdout)\n","\n","# Format and export data\n","predictions_table = pd.DataFrame(mini_holdout_predictions, columns=['predictions'])\n","predictions_table.to_csv('team5-module4-predictions.csv', index=False)\n","print(predictions_table.value_counts())"]}],"metadata":{"colab":{"provenance":[{"file_id":"1e9XPk5LcGiWXryruFBLPYmB9aHkEMCQm","timestamp":1718168555429},{"file_id":"1P5knhNJXWWrEy3FL8sQx7RgarQYYKoxn","timestamp":1718051865631},{"file_id":"https://github.com/byui-cse/cse450-course/blob/master/notebooks/starter_bikes.ipynb","timestamp":1717619415586}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}